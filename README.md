

\# Smart Face Recognition System with Liveness Detection

A robust, modular, and privacy-focused face recognition system built with Python, OpenCV, and Dlib. This project goes beyond simple matching by implementing **\*\*Smart Liveness Detection\*\*** (Blink \+ Head Movement tracking) to prevent photo spoofing attacks.

\!\[Python\](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge\&logo=python)  
\!\[OpenCV\](https://img.shields.io/badge/OpenCV-Computer%20Vision-green?style=for-the-badge\&logo=opencv)  
\!\[License\](https://img.shields.io/badge/License-Unlicense-lightgrey?style=for-the-badge)

\#\# üìñ Table of Contents  
\- \[About the Project\](\#-about-the-project)  
\- \[Directory Structure\](\#-directory-structure)  
\- \[Key Features\](\#-key-features)  
\- \[Technical Logic (Liveness)\](\#-technical-logic-how-it-works)  
\- \[Installation & Setup\](\#-installation--setup)  
\- \[Usage Guide\](\#-usage-guide)  
\- \[Configuration\](\#-configuration)  
\- \[Troubleshooting\](\#-troubleshooting)  
\- \[License\](\#-license)

\---

\#\# üî≠ About the Project

This system is designed to identify users in real-time while ensuring they are physically present. Unlike standard face recognition scripts that can be tricked by holding up a photo, this system requires active verification via **\*\*facial gestures\*\***.

It uses a modular architecture where the core logic is separated from the execution scripts, making it scalable and easy to maintain.

\#\# üìÇ Directory Structure

The repository is organized to separate the source code from the project root files.

\`\`\`text  
face\_recognition/                \# Repository Root  
‚îú‚îÄ‚îÄ face\_recognition\_system/     \# Main Codebase  
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  \# Entry Point (CLI Controller)  
‚îÇ   ‚îú‚îÄ‚îÄ data/                    \# Local Storage (GitIgnored)  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw/                 \# Captured User Images  
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/              \# Trained encodings.pickle  
‚îÇ   ‚îî‚îÄ‚îÄ src/                     \# Source Package  
‚îÇ       ‚îú‚îÄ‚îÄ \_\_init\_\_.py  
‚îÇ       ‚îú‚îÄ‚îÄ config.py            \# Central Configuration  
‚îÇ       ‚îú‚îÄ‚îÄ data\_loader.py       \# Dataset Creation Logic  
‚îÇ       ‚îú‚îÄ‚îÄ encoder.py           \# Model Training Logic  
‚îÇ       ‚îú‚îÄ‚îÄ recognition.py       \# Real-time Core System  
‚îÇ       ‚îî‚îÄ‚îÄ utils.py             \# File System Helpers  
‚îú‚îÄ‚îÄ .gitignore                   \# Privacy protection  
‚îú‚îÄ‚îÄ requirements.txt             \# Dependencies  
‚îú‚îÄ‚îÄ UNLICENSE                    \# Public Domain License  
‚îî‚îÄ‚îÄ README.md                    \# Documentation

## **‚ú® Key Features**

### **1\. Modular Architecture**

Code is organized into a src package. Logic, configuration, and execution are decoupled, making the codebase clean and professional.

### **2\. Multi-Factor Liveness Detection**

To mark a user as "Live" (Green Box), they must perform **one** of the following actions:

* **Blink:** Detected via Eye Aspect Ratio (EAR).  
* **Head Movement:** Detected by tracking the Euclidean distance of the nose tip vector across frames.

### **3\. Floating UI Design**

Instead of obstructive solid boxes, the system uses a modern "floating text" design with high-contrast outlines (strokes), ensuring readability on both dark and light backgrounds.

### **4\. Smart State Latching**

Once a liveness action (Blink/Move) is detected, the system "remembers" the live status for a configurable duration (default: 2 seconds). This prevents the UI from flickering between "Live" and "Spoof" while the user is interacting normally.

## ---

**üß† Technical Logic: How It Works**

### **A. Face Encoding**

We use the **HOG (Histogram of Oriented Gradients)** model via dlib to detect faces. The face is then transformed into a **128-dimensional vector** (embedding). When identifying a user, we calculate the Euclidean distance between the live face vector and our known database.

### **B. Blink Detection (EAR)**

We map 6 landmarks per eye. The Eye Aspect Ratio (EAR) is calculated using the formula:

$$EAR \= \\frac{||p2 \- p6|| \+ ||p3 \- p5||}{2 \\times ||p1 \- p4||}$$

* If the EAR falls below 0.24 (eyes closed) for 2 consecutive frames, a blink is registered.

### **C. Movement Detection (Nose Tracking)**

We track the **Nose Tip** landmark (Index 30 in the 68-point model).

* The system calculates the Euclidean distance of the nose tip between the current frame and the previous frame.  
* If the distance \> 2.5 pixels, it registers as intentional movement (nodding/turning).

## ---

**üöÄ Installation & Setup**

### **1\. Clone the Repository**

Bash

git clone \[https://github.com/HrushiSanap/face\_recognition.git\](https://github.com/HrushiSanap/face\_recognition.git)  
cd face\_recognition

### **2\. Install Dependencies**

**Note:** This project requires dlib, which compiles C++ code. You may need CMake installed (pip install cmake).

Bash

pip install \-r requirements.txt

*If you face errors installing dlib on Windows, install [Visual Studio C++ Build Tools](https://www.google.com/search?q=https://visualstudio.microsoft.com/visual-cpp-build-tools/) first.*

## ---

**üõ† Usage Guide**

All commands are run using the central main.py controller located inside the face\_recognition\_system folder.

**First, navigate to the code folder:**

Bash

cd face\_recognition\_system

### **Step 1: Capture User Data**

This will open your webcam. Look at the camera and slowly rotate your head to capture different angles.

Bash

python main.py capture \--name "YourName"

* **Controls:** Press q to quit early.  
* **Output:** Images are saved in data/raw/YourName/.

### **Step 2: Train the Model**

This processes the images and creates the facial embeddings.

Bash

python main.py train

* **Output:** Creates data/models/encodings.pickle.

### **Step 3: Start Recognition**

Run the main security system.

Bash

python main.py run

* **Visual Indicators:**  
  * üî¥ **Red:** Unknown Face.  
  * üü° **Yellow/Cyan:** Known Face, but static (Potential Spoof).  
  * üü¢ **Green:** Known Face \+ Live (Blink/Move verified).  
* **Controls:** Press q to exit.

## ---

**‚öôÔ∏è Configuration**

You can fine-tune the system sensitivities in src/config.py.

| Parameter | Default | Description |
| :---- | :---- | :---- |
| EYE\_AR\_THRESH | 0.24 | Threshold for "eyes closed". Lower if it detects blinks too easily. |
| EYE\_AR\_CONSEC\_FRAMES | 2 | How many frames eyes must be closed to count as a blink. |
| MOVEMENT\_THRESHOLD | 2.5 | Minimum pixel distance nose must move to trigger "Live". |
| LIVENESS\_TIMEOUT | 2.0 | Seconds the "Live" status persists after a gesture. |
| RECOGNITION\_INTERVAL | 5 | Runs heavy face recognition every N frames to save CPU. |

## ---

**‚ùì Troubleshooting**

**1\. ModuleNotFoundError: No module named 'src'**

* Ensure you are running the command from *inside* the face\_recognition\_system directory, not the root repo folder.

**2\. dlib installation fails**

* Windows: Install "Desktop development with C++" workload via Visual Studio Build Tools.  
* Linux: Run sudo apt-get install build-essential cmake.

**3\. System is lagging**

* Open src/config.py and increase FRAME\_RESIZE\_SCALE (e.g., make it smaller, like 0.20) or increase RECOGNITION\_INTERVAL to 10\.

## ---

**üìú License**

This project is released under the The Unlicense.  
This means the code is dedicated to the public domain. You are free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means.  
For more information, please refer to [http://unlicense.org/](http://unlicense.org/)